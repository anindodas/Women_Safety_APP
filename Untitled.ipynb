{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c951876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No faces detected in the image\n",
      "No faces detected in the image\n",
      "No faces detected in the image\n",
      "No faces detected in the image\n",
      "No faces detected in the image\n",
      "No faces detected in the image\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21012\\2086060426.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputImage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error: Failed to capture frame\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def getFace(faceDetectionModel, inputImage, conf_threshold=0.7):\n",
    "    cpy_input_image = inputImage.copy()  # To avoid modifications to the original input\n",
    "\n",
    "    frameWidth = cpy_input_image.shape[1]\n",
    "    frameHeight = cpy_input_image.shape[0]\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(cpy_input_image, scalefactor=1, size=(227, 227), mean=(104, 117, 123), crop=False)  # preprocessed image\n",
    "\n",
    "    faceDetectionModel.setInput(blob)\n",
    "    detections = faceDetectionModel.forward()\n",
    "\n",
    "    bounding_boxes = []\n",
    "    for i in range(detections.shape[2]):  # detections is an array having [no.of.images/batch size, classes/channels, i-th detections, confidence_score]\n",
    "        confidence_score = detections[0, 0, i, 2]  # gets the confidence score for i-detections, 4-th index(value:2) shows confidence score\n",
    "\n",
    "        if confidence_score > conf_threshold:  # get the co-ordinates of the bounding boxes only if its detected as a face, confidence score sets the minimum limit\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bounding_boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "            cv2.rectangle(cpy_input_image, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    return cpy_input_image, bounding_boxes\n",
    "\n",
    "# loading the face detection model\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "\n",
    "# loading the gender detection pretrained model\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "# realtime\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, inputImage = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame\")\n",
    "        break\n",
    "\n",
    "    detected_image, bounding_boxes = getFace(faceNet, inputImage)\n",
    "\n",
    "    if not bounding_boxes:\n",
    "        print(\"No faces detected in the image\")\n",
    "    else:\n",
    "        for bounding_box in bounding_boxes:\n",
    "            x1, y1, x2, y2 = bounding_box\n",
    "            detected_face_box = inputImage[y1:y2, x1:x2]\n",
    "\n",
    "            detected_face_blob = cv2.dnn.blobFromImage(detected_face_box, scalefactor=1, size=(227, 227), mean=([78.4263377603, 87.7689143744, 114.895847746]), crop=False)\n",
    "\n",
    "            genderNet.setInput(detected_face_blob)\n",
    "            genderPrediction = genderNet.forward()\n",
    "\n",
    "            gender = genderList[genderPrediction[0].argmax()]\n",
    "\n",
    "            cv2.putText(img=detected_image, text=gender, org=(x1, y1 - 10), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=1.1, color=(0,255,0), thickness=2)\n",
    "        \n",
    "        cv2.imshow(\"Detected Image\", detected_image)\n",
    "       \n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "print(f\"Predicted Gender: {gender}, Confidence Score: {genderPrediction[0].max()}\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b804ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
